{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = '../data/unzipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf __pycache__\n",
      "rm -f ../data/unzipped/*\n",
      "rm -f *.pyc\n",
      "rm -f .DS_Store\n",
      "rm -f ../.DS_Store\n",
      "rm -f ../data/.DS_Store\n",
      "venv/bin/python3 clean.py \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hudsonnash/Dropbox/Mac/Desktop/Su22s/phil/phil/control/venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./convert.sh ../data/unzipped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='make clean; make run', returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import run\n",
    "run('make clean; make run',shell=True) # when run excludes extract() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-ManilaCity2021_Part3-Status_of_PYs_Recomm.docx',\n",
       " '02-LamitanCity2010_Status_of_Prior_Years_Audit_Recommendations.doc',\n",
       " '02-LamitanCity2010_Status_of_Prior_Years_Audit_Recommendations.pdf',\n",
       " '10-ManilaCity2021_Part3-Status_of_PYs_Recomm.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extracttools import *\n",
    "di = '../data/unzipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/78/kn36tmx96wqdrmwtp93_zxg40000gn/T/ipykernel_36659/2483136259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import config as conf\n",
    "def add_date_cols(df,fn): # add file download date from metadata and other dates from metadata and date or year published\n",
    "    pass # TODO: 11/30/22\n",
    "def add_location_col(df,fn): # add location from filename\n",
    "    pass # TODO: 11/30/22\n",
    "def good_match(og:str,ref:list[str]):\n",
    "    good = ''\n",
    "    approx = 0\n",
    "    og = og.lower()\n",
    "    for target in ref:\n",
    "        new_approx = fuzz.token_sort_ratio(og,target)\n",
    "        if new_approx > approx:\n",
    "            approx = new_approx\n",
    "            good = target\n",
    "    if approx < 60:\n",
    "        good = og\n",
    "    return good\n",
    "def polish(df):\n",
    "    # remove \\r characters\n",
    "    df = df.rename(lambda s:s.replace('\\r',' '),axis='columns') # remove '\\r' from headers\n",
    "    df = df.replace('\\r','',regex=True) # replace all '\\r' throughout\n",
    "    # approximate headers\n",
    "    cols = df.columns\n",
    "    newcols = [good_match(col,conf.CANON_HEADERS) for col in cols]\n",
    "    df.columns = newcols\n",
    "    return df\n",
    "def overflow_check(df,ref,main_df):\n",
    "    head = df.head(10) # Can set this to be the necessary number of rows -- typically overflow only lasts for 2-3 rows\n",
    "    headers_at = 0\n",
    "    for row in range(len(head)):\n",
    "        if sum([bool(any([fuzz.token_sort_ratio(cell,target) > 60 for target in ref])) for cell in head.loc[row]]) < 2: # if there are less than 2 near-canonical headers\n",
    "            if sum([bool(any([fuzz.token_sort_ratio(cell,target) > 60 for target in ref])) for cell in head.loc[row]]) > 4: # if there are more than 4 near-canonical headers in the row\n",
    "                headers_at = row\n",
    "                overflow = df.columns.values\n",
    "                df = polish(df)\n",
    "                overflow = pd.DataFrame(overflow,columns = ref)\n",
    "                main_df = main_df.append(overflow,ignore_index=True)\n",
    "                df = df.loc[:headers_at+1]\n",
    "            df = polish(df)\n",
    "            df.loc[headers_at,'audit observation'] = '!PHIL EXCEPTION: Dataframe columns are faulty and data has been lost. Refer to the document to find the source of error and report the bug in the repository: https://github.com/hudnash/phil/issues'\n",
    "def extract(di):\n",
    "    target_page = -1\n",
    "    table_end_page = -1\n",
    "    out = pd.DataFrame()\n",
    "    file_counter = 1\n",
    "    for filename in os.listdir(di):\n",
    "        print('Looking for file n = '+str(file_counter)+'...')\n",
    "        no_df_found = True\n",
    "        if os.path.splitext(filename)[1] == \".pdf\":\n",
    "            filename = os.path.join(di,filename)\n",
    "            print('Now scanning '+filename)\n",
    "            pdf = p.PdfFileReader(filename)\n",
    "            page_count = pdf.numPages\n",
    "            for pagenum in range(page_count):\n",
    "                print('Looking for Part 3 on p. '+str(pagenum+1)+' of '+str(page_count)+' pages')\n",
    "                page = pdf.getPage(pagenum)\n",
    "                page_content = page.extractText()\n",
    "                if all([target in page_content.lower() for target in conf.TARGET_SENTENCE]):\n",
    "                    no_df_found = False\n",
    "                    target_page = pagenum\n",
    "                    delete_pages_before(filename,target_page)\n",
    "                    dfs = default(filename) # feed chunks of PDF table scraped via tabula/io.py\n",
    "                    if len(dfs) > 1:\n",
    "                        count = 1\n",
    "                        for df in dfs:\n",
    "                            print(str(count)+' of '+str(len(dfs))+' dataframes collected.')\n",
    "                            df = polish(df)\n",
    "                            overflow_check(df,conf.CANON_HEADERS,out)\n",
    "                            #df = add_filename_col(filename,df) # add a filename column\n",
    "                            #df = add_location_col(df,filename)\n",
    "                            #df = add_date_cols(df,filename)\n",
    "                            out = out.append(df,ignore_index=True) #TODO 11/30/22: Approximate header string matching during append\n",
    "                            count = count + 1\n",
    "                    elif len(dfs) == 1:\n",
    "                        df = add_filename_col(filename,dfs[0])\n",
    "                        df = polish(df)\n",
    "                        overflow_check(df,conf.CANON_HEADERS,out)\n",
    "                        out = out.append(df,ignore_index=True) #TODO 11/30/22: Approximate header string matching during append\n",
    "                        print('1 of 1 dataframe collected.')\n",
    "        if no_df_found:\n",
    "            print('Exception: Could not locate tables in file #'+str(file_counter)+' '+filename+'\\nTry: Adjust target sequences list in config.py')\n",
    "        file_counter = file_counter + 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = df.head(5)\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
